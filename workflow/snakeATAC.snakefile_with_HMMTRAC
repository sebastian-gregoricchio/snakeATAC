from typing import List
import pathlib
import re
import numpy

# Function to handle the values for the wilcards
def constraint_to(values: List[str]) -> str:
    """
    From a list, return a regular expression allowing each
    value and not other.
    ex: ["a", "b", "v"] -> (a|b|v)
    """
    if isinstance(values, str):
        raise ValueError("constraint_to(): Expected a list, got str instead")

    return "({})".format("|".join(values))

# working diirectory
workdir: config["output_directory"]


# get the unique samples names and other variables
FILENAMES = next(os.walk(config["runs_directory"]))[2]
RUNNAMES = [re.sub(rf"{config["fastq_extension"]}$", "", i) for i in FILENAMES]
SAMPLENAMES = numpy.unique([re.sub(rf"{config["runs_suffix"][0]}|{config["runs_suffix"][1]}.*$", "", i) for i in RUNNAMES])

if (eval(str(config["perform_HMCan_correction"])) == True):
    BINS=config["smallBinLength"]
else:
    BINS=config["bigWig_binSize"]


if (eval(str(config["call_summits"])) == True):
    SUMMITS="--call-summits"
else:
    SUMMITS=""


if (config["perform_HMCan_correction"] == True):
    PEAKSDIR = "04_Normalization/HMCan_output/"
    SUMMARYDIR = "05_Overall_quality_and_info/"
    PEAKCALLER = "HMCAN"
else:
    PEAKSDIR = "05_Peaks_MACS2/"
    PEAKSHMMRATAC = "06_Peaks_HMMRATAC/"
    SUMMARYDIR = "07_Overall_quality_and_info/"
    PEAKCALLER = "MACS2"


# generation of global wildcard_constraints
wildcard_constraints:
    RUNS=constraint_to(RUNNAMES),
    SAMPLES=constraint_to(SAMPLENAMES)



# Generate optional inputs for HMCan (1) vs Sequencing depth normalization
norm_outputs = []
if (eval(str(config["perform_HMCan_correction"]))):
    norm_outputs.append("04_Normalization/HMCan_output/CONFIGURATION_file_HMCan.txt") #HMCan_config
    norm_outputs.append(expand(os.path.join("04_Normalization/HMCan_output/", ''.join(["{sample}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_regions.bed"])), sample=SAMPLENAMES)) # HMCan_regions
    norm_outputs.append(expand(os.path.join("04_Normalization/HMCan_output/", ''.join(["{sample}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_peaks.narrowPeak"])), sample=SAMPLENAMES)) # HMCan_peaks
    norm_outputs.append(expand(os.path.join("04_Normalization/HMCan_output/", ''.join(["{sample}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_CNV_profile.txt"])), sample=SAMPLENAMES)) # HMCan_CNV_profile

else:
    norm_outputs.append(expand(os.path.join("05_Peaks_MACS2/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_FDR{fdr}_peaks.xls"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), fdr=str(config["FDR_cutoff"]))) # peaks_xls MACS2
    norm_outputs.append(expand(os.path.join("05_Peaks_MACS2/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_FDR{fdr}_peaks.narrowPeak"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), fdr=str(config["FDR_cutoff"]))) # narrowPeaks MACS2
    norm_outputs.append(expand(os.path.join("06_Peaks_HMMRATAC/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_HMMRATAC_peaks.gappedPeak"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))) # gappedPeaks HMMRATAC
    norm_outputs.append(os.path.join(SUMMARYDIR, "Heatmap_on_rawScores_for_HMMRATAC.peaks_union_population.pdf")) # correlations with HMMRATAC

# ========================================================================================
#  - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -
# ========================================================================================



# ========================================================================================
# Function to run all funtions
rule AAA_initialization:
    input:
        # Rule A
        build_fastqcDir = ancient(os.path.dirname("01_fastQC_raw/multiQC_raw/")),
        build_BAM = ancient(os.path.dirname("02_BAM/flagstat/")),
        build_BAMdedup_metrics = ancient(os.path.dirname("03_BAM_dedup/metrics/")),
        build_BAMdedup_unshifted = ancient(os.path.dirname("03_BAM_dedup/unshifted_bams")),
        build_BAMdedup_flagstat = ancient(os.path.dirname("03_BAM_dedup/flagstat/")),
        build_BAMdedup_multiQC = ancient(os.path.dirname("03_BAM_dedup/fastQC/multiQC_dedup_bams/")),
        build_BAMdedup_fragmentPlots = ancient(os.path.dirname("03_BAM_dedup/fragmentSizeDistribution_plots/")),
        build_normalization = ancient(os.path.dirname("04_Normalization/scalingFactor/")),
        build_normBigWig = ancient(os.path.dirname("04_Normalization/normalized_bigWigs/")),
        build_summary_directory = ancient(os.path.dirname(SUMMARYDIR)),

        # Rule B
        fastQC_raw_zip = ancient(expand(os.path.join("01_fastQC_raw", "{run}_fastqc.zip"), run=RUNNAMES)),

        # Rule C
        multiQC_raw_html = ancient("01_fastQC_raw/multiQC_raw/multiQC_report_fastqRaw.html"),

        # Rule D
        #SAM = ancient(expand(os.path.join("01b_SAM_tempFolder/", "{sample}.sam"), sample=SAMPLENAMES)),

        # Rule E
        filtBAM_sorted_woMT = ancient(expand(os.path.join("02_BAM/", "{sample}_mapQ{MAPQ}_sorted_woMT.bam"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))),
        filtBAM_sorted_woMT_index = ancient(expand(os.path.join("02_BAM/", "{sample}_mapQ{MAPQ}_sorted_woMT.bam.bai"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))),
        flagstat_unfiltered_BAM = ancient(expand(os.path.join("02_BAM/flagstat/", "{sample}_flagstat_UNfiltered_bam.txt"), sample=SAMPLENAMES)),
        flagstat_on_filtered_woMT_BAM = ancient(expand(os.path.join("02_BAM/flagstat/", "{sample}_flagstat_filtered_bam_woMT.txt"), sample=SAMPLENAMES)),

        # Rule F
        dedup_BAM_metrics = ancient(expand(os.path.join("03_BAM_dedup/metrics", "{sample}_metrics_woMT_dedup_bam.txt"), sample=SAMPLENAMES)),
        dedup_BAM_flagstat = ancient(expand(os.path.join("03_BAM_dedup/flagstat/", "{sample}_flagstat_filtered_bam_woMT_dedup.txt"), sample=SAMPLENAMES)),

        # Rule G
        dedup_BAM_shifted_sorted = ancient(expand(os.path.join("03_BAM_dedup/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_sorted.bam"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))),
        dedup_BAM_shifted_sorted_index = ancient(expand(os.path.join("03_BAM_dedup/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_sorted.bam.bai"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))),
        dedup_BAM_flagstat_shifted_sorted = ancient(expand(os.path.join("03_BAM_dedup/flagstat/", "{sample}_flagstat_woMT_dedup_shifted_sorted.txt"), sample=SAMPLENAMES)),

        # Rule H
        fastQC_zip_BAM = ancient(expand(os.path.join("03_BAM_dedup/fastQC/", "{sample}_mapQ{MAPQ}_sorted_woMT_dedup_fastqc.zip"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))),

        # Rule I
        multiQC_BAM_html = ancient("03_BAM_dedup/fastQC/multiQC_dedup_bams/multiQC_report_BAMs_dedup.html"),

        # Rule J
        fragmentSizePlot = ancient(expand(os.path.join("03_BAM_dedup/fragmentSizeDistribution_plots/", "{sample}_fragment_size_distribution.pdf"), sample=SAMPLENAMES)),

        # Rule K
        scalingFactors_txt_result = "04_Normalization/scalingFactor/scalingFactor_results.txt",

        # Rule_normalization
        norm_bw = ancient(expand(os.path.join("04_Normalization/normalized_bigWigs/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_normalized_bs{binSize}.bw"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), binSize=str(BINS))),

        # Rule peakCalling
        norm_outputs = norm_outputs,

        # Rule Z1 counts_summary
        temp_file_counts = ancient(expand(os.path.join(SUMMARYDIR, "{sample}_counts_summary.temp"), sample=SAMPLENAMES)),

        # Rule Z2 PCA and plotCorrelation
        PCA = ancient(os.path.join(SUMMARYDIR, "PCA_on_BigWigs_wholeGenome.pdf")),
        hetamap_spearman = ancient(os.path.join(SUMMARYDIR, "Heatmap_on_BigWigs_wholeGenome_spearmanMethod.pdf")),
        hetamap_pearson = ancient(os.path.join(SUMMARYDIR, "Heatmap_on_BigWigs_wholeGenome_pearsonMethod.pdf")),
        scatterplot_spearman = ancient(os.path.join(SUMMARYDIR, "Scatterplot_on_BigWigs_wholeGenome_spearmanMethod.pdf")),
        scatterplot_pearson = ancient(os.path.join(SUMMARYDIR, "Scatterplot_on_BigWigs_wholeGenome_pearsonMethod.pdf")),

        # Rule Z4 Heatmap zScores peaks
        rawScores_hetamap_MACS2 = ancient(os.path.join(SUMMARYDIR, "".join(["Heatmap_on_rawScores_for_", PEAKCALLER, ".peaks_union_population.pdf"])))

    params:
        summary_file = str(os.path.join(SUMMARYDIR, "counts_summary.txt"))

    shell:
        """
        mkdir -p 04_Normalization/HMCan_output/temp_GENOME
        rm -r 04_Normalization/HMCan_output/temp_GENOME

        uniq -u {params.summary_file} > summary_file.temp
        (head -n 1 summary_file.temp && tail -n +2 summary_file.temp | sort -k 1) > {params.summary_file}
        rm summary_file.temp

        mkdir -p 01b_SAM_tempFolder
        rm -R 01b_SAM_tempFolder

        pdfcombine 03_BAM_dedup/fragmentSizeDistribution_plots/*_fragment_size_distribution.pdf -o 03_BAM_dedup/fragmentSizeDistribution_plots/ALL.SAMPLES_fragmentSizeDistribution_plots.pdf -sf

        printf '\033[1;36mPipeline ended!\\n\033[0m'
        """
# ========================================================================================


# ----------------------------------------------------------------------------------------
# generate all the required folders
rule A_build_environment:
    output:
        build_fastqcDir = os.path.dirname("01_fastQC_raw/multiQC_raw/"),
        build_BAM = os.path.dirname("02_BAM/flagstat/"),
        build_BAMdedup_metrics = os.path.dirname("03_BAM_dedup/metrics/"),
        build_BAMdedup_unshifted = os.path.dirname("03_BAM_dedup/unshifted_bams/"),
        build_BAMdedup_flagstat = os.path.dirname("03_BAM_dedup/flagstat/"),
        build_BAMdedup_multiQC = os.path.dirname("03_BAM_dedup/fastQC/multiQC_dedup_bams/"),
        build_BAMdedup_fragmentPlots = os.path.dirname("03_BAM_dedup/fragmentSizeDistribution_plots/"),
        build_normalization = os.path.dirname("04_Normalization/scalingFactor/"),
        build_normBigWig = os.path.dirname("04_Normalization/normalized_bigWigs/"),
        build_summary_directory = os.path.dirname(SUMMARYDIR)
    shell:
        """
        printf '\033[1;36mBuilding folders tree...\\n\033[0m'

        mkdir -p {output.build_fastqcDir}
        mkdir -p {output.build_BAM}
        mkdir -p {output.build_BAMdedup_metrics}
        mkdir -p {output.build_BAMdedup_unshifted}
        mkdir -p {output.build_BAMdedup_flagstat}
        mkdir -p {output.build_BAMdedup_multiQC}
        mkdir -p {output.build_BAMdedup_fragmentPlots}
        mkdir -p {output.build_normalization}
        mkdir -p {output.build_normBigWig}
        mkdir -p {output.build_summary_directory}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Perform the FastQC on raw fastq.gz
rule B_fastQC_raw:
    input:
        build_fastqcDir = ancient(os.path.dirname("01_fastQC_raw/multiQC_raw/")),
        fastq_gz = ancient(os.path.join(config["runs_directory"], "{RUNS}.fastq.gz"))
    output:
        html = os.path.join("01_fastQC_raw","{RUNS}_fastqc.html"),
        zip =  os.path.join("01_fastQC_raw","{RUNS}_fastqc.zip")
    params:
        fastQC_raw_outdir = os.path.join(config["output_directory"], "01_fastQC_raw"),
        run = "{RUNS}",
        CPUs = config["fastQC_threads"]
    threads:
        config["fastQC_threads"]
    shell:
        """
        printf '\033[1;36m{params.run}: Performing fastQC on raw fastq...\\n\033[0m'
        fastqc -t {params.CPUs} --outdir {params.fastQC_raw_outdir} {input.fastq_gz}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Perform multiQC for raw fastq reports
rule C_multiQC_raw:
    input:
        fastqc_zip = ancient(expand(os.path.join("01_fastQC_raw", "{run}_fastqc.zip"), run=RUNNAMES))
    output:
        multiqcReportRaw = "01_fastQC_raw/multiQC_raw/multiQC_report_fastqRaw.html"
    params:
        fastqc_Raw_reports = os.path.join("01_fastQC_raw", "*.zip"),
        multiQC_raw_outdir = os.path.join(config["output_directory"], "01_fastQC_raw/multiQC_raw/")
    shell:
        """
        printf '\033[1;36mGenerating multiQC report for fatsq quality test...\\n\033[0m'
        multiqc -f --outdir {params.multiQC_raw_outdir} -n multiQC_report_fastqRaw.html {params.fastqc_Raw_reports}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Reads alignement
rule D_bwa_align:
    input:
        build_SAM = "01b_SAM_tempFolder/",
        R1 = ancient(os.path.join(config["runs_directory"], "{SAMPLES}_R1.fastq.gz")),
        R2 = ancient(os.path.join(config["runs_directory"], "{SAMPLES}_R2.fastq.gz"))
    output:
        SAM = os.path.join("01b_SAM_tempFolder/", "{SAMPLES}.sam")
    params:
        genome = os.path.join(config["genome_folder"], "*.fa"),
        sample = "{SAMPLES}",
        CPUs = config["bwa_threads"]
    threads:
        config["bwa_threads"]
    shell:
        """
        mkdir -p {input.build_SAM}
        printf '\033[1;36m{params.sample}: alignment of the reads...\\n\033[0m'
        bwa mem -t {params.CPUs} {params.genome} {input.R1} {input.R2} > {output.SAM}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# SAM filtering for mapping quality and BAM generation | BAM MT-reads removal
rule E_sam_to_bam:
    input:
        SAM = ancient(os.path.join("01b_SAM_tempFolder/", "{SAMPLES}.sam"))
    output:
        filtBAM = temp(os.path.join("02_BAM/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), ".bam"]))),
        filtBAM_sorted = temp(os.path.join("02_BAM/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted.bam"]))),
        filtBAM_sorted_index = temp(os.path.join("02_BAM/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted.bam.bai"]))),
        flagstat_on_unfiltered_BAM = os.path.join("02_BAM/flagstat/", "{SAMPLES}_flagstat_UNfiltered_bam.txt"),
        filtBAM_sorted_woMT = os.path.join("02_BAM/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT.bam"])),
        filtBAM_sorted_woMT_index = os.path.join("02_BAM/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT.bam.bai"])),
        flagstat_on_filtered_woMT_BAM = os.path.join("02_BAM/flagstat/", "{SAMPLES}_flagstat_filtered_bam_woMT.txt")
    params:
        mapq_cutoff = str(config["mapQ_cutoff"]),
        sample = "{SAMPLES}",
        CPUs = config["SAMtools_threads"]
    threads:
        config["SAMtools_threads"]
    shell:
        """
        printf '\033[1;36m{params.sample}: filtering SAM and generate BAM...\\n\033[0m'
        samtools view -@ {params.CPUs} -b -q {params.mapq_cutoff} {input.SAM} -o {output.filtBAM}

        printf '\033[1;36m{params.sample}: sorting BAM...\\n\033[0m'
        samtools sort -@ {params.CPUs} {output.filtBAM} -o {output.filtBAM_sorted}
        samtools index -@ {params.CPUs} -b {output.filtBAM_sorted} {output.filtBAM_sorted_index}

        printf '\033[1;36m{params.sample}: Getting flagstat from unfiltered BAM...\\n\033[0m'
        samtools flagstat {output.filtBAM_sorted} -@ {params.CPUs} > {output.flagstat_on_unfiltered_BAM}

        printf '\033[1;36m{params.sample}: Removing MT reads from BAM...\\n\033[0m'
        samtools idxstats {output.filtBAM_sorted} | cut -f 1 | grep -v M | xargs samtools view -@ {params.CPUs} -b {output.filtBAM_sorted} > {output.filtBAM_sorted_woMT}
        samtools index -@ {params.CPUs} -b {output.filtBAM_sorted_woMT} {output.filtBAM_sorted_woMT_index}

        printf '\033[1;36m{params.sample}: Getting flagstat from BAM without MT-DNA...\\n\033[0m'
        samtools flagstat {output.filtBAM_sorted_woMT} -@ {params.CPUs} > {output.flagstat_on_filtered_woMT_BAM}

        rm {input.SAM}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# BAM duplicates removal and relative flagstat | BAM reads shifting
rule F_bam_deduplication:
    input:
        BAM = ancient(os.path.join("02_BAM/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT.bam"])))
    output:
        dedup_BAM = os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bam"])),
        dedup_BAM_index = os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bai"])),
        dedup_BAM_metrics = os.path.join("03_BAM_dedup/metrics", "{SAMPLES}_metrics_woMT_dedup_bam.txt"),
        dedup_BAM_flagstat = os.path.join("03_BAM_dedup/flagstat/", "{SAMPLES}_flagstat_filtered_bam_woMT_dedup.txt")
    params:
        sample = "{SAMPLES}",
        minFragmentLength = config["minFragmentLength"],
        maxFragmentLength = config["maxFragmentLength"],
        rm_dup = config["remove_duplicates"],
        CPUs = config["SAMtools_threads"]
    threads:
        config["SAMtools_threads"]
    shell:
        """
        printf '\033[1;36m{params.sample}: Removing BAM duplicates...\\n\033[0m'
        picard MarkDuplicates CREATE_INDEX=true INPUT={input.BAM} OUTPUT={output.dedup_BAM} METRICS_FILE={output.dedup_BAM_metrics} ASSUME_SORT_ORDER=coordinate REMOVE_DUPLICATES={params.rm_dup} VALIDATION_STRINGENCY=STRICT
        samtools flagstat {output.dedup_BAM} -@ {params.CPUs} > {output.dedup_BAM_flagstat}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# BAM reads shifting
rule G_bam_shifting:
    input:
        dedup_BAM = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bam"]))),
        dedup_BAM_index = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bai"])))
    output:
        dedup_BAM_shifted_toSort = temp(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup_shifted.ToSort.bam"]))),
        dedup_BAM_shifted_sorted = os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam"])),
        dedup_BAM_shifted_sorted_index = os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam.bai"])),
        dedup_BAM_shifted_sorted_flagstat = os.path.join("03_BAM_dedup/flagstat/", "{SAMPLES}_flagstat_woMT_dedup_shifted_sorted.txt")
    params:
        sample = "{SAMPLES}",
        minFragmentLength = str(config["minFragmentLength"]),
        maxFragmentLength = str(config["maxFragmentLength"]),
        CPUs = config["SAMtools_threads"]
    threads:
        config["SAMtools_threads"]
    shell:
        """
        printf '\033[1;36m{params.sample}: Shifting reads in BAM...\\n\033[0m'
        alignmentSieve -p {params.CPUs} --ATACshift --bam {input.dedup_BAM} --outFile {output.dedup_BAM_shifted_toSort} --minFragmentLength {params.minFragmentLength} --maxFragmentLength {params.maxFragmentLength}

        printf '\033[1;36m{params.sample}: Sorting shifted BAM...\\n\033[0m'
        samtools sort -@ {params.CPUs} {output.dedup_BAM_shifted_toSort} -o {output.dedup_BAM_shifted_sorted}
        samtools index -@ {params.CPUs} -b {output.dedup_BAM_shifted_sorted} {output.dedup_BAM_shifted_sorted_index}

        printf '\033[1;36m{params.sample}: Getting flagstat from shifted BAM...\\n\033[0m'
        samtools flagstat {output.dedup_BAM_shifted_sorted} -@ {params.CPUs} > {output.dedup_BAM_shifted_sorted_flagstat}

        echo '------------------------------------------------------------------------'
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# FastQC on BAMs
rule H_fastQC_BAMs:
    input:
        dedup_BAM = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bam"]))),
        dedup_BAM_index = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bai"])))
    output:
        html = os.path.join("03_BAM_dedup/fastQC/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup_fastqc.html"])),
        zip = os.path.join("03_BAM_dedup/fastQC/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup_fastqc.zip"]))
    params:
        fastQC_BAMs_outdir = os.path.join(config["output_directory"], "03_BAM_dedup/fastQC/"),
        sample = "{SAMPLES}",
        CPUs = config["fastQC_threads"]
    threads:
        config["fastQC_threads"]
    shell:
        """
        printf '\033[1;36m{params.sample}: Performing fastQC on deduplicated bam...\\n\033[0m'
        fastqc -t {params.CPUs} --outdir {params.fastQC_BAMs_outdir} {input.dedup_BAM}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Perform multiQC for BAMs
rule I_multiQC_BAMs:
    input:
        BAM_fastqc_zip = ancient(expand(os.path.join("03_BAM_dedup/fastQC/", "{sample}_mapQ{MAPQ}_sorted_woMT_dedup_fastqc.zip"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"])))
    output:
        multiqcReportBAM = "03_BAM_dedup/fastQC/multiQC_dedup_bams/multiQC_report_BAMs_dedup.html"
    params:
        fastQC_BAM_reports = os.path.join("03_BAM_dedup/fastQC/", "*.zip"),
        multiQC_BAM_outdir = os.path.join(config["output_directory"], "03_BAM_dedup/fastQC/multiQC_dedup_bams/")
    shell:
        """
        printf '\033[1;36mGenerating multiQC report from deduplicated bam quality test...\\n\033[0m'
        multiqc -f --outdir {params.multiQC_BAM_outdir} -n multiQC_report_BAMs_dedup.html {params.fastQC_BAM_reports}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Perform fragment size distribution plot
rule J_fragment_size_distribution:
    input:
        BAM = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bam"]))),
        BAM_index = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bai"])))
    output:
        plot = os.path.join("03_BAM_dedup/fragmentSizeDistribution_plots/", "{SAMPLES}_fragment_size_distribution.pdf")
    params:
        sample = "{SAMPLES}",
        plotFormat = config["plot_format"],
        binSize = str(config["window_length"]),
        blacklist = config["blacklist_file"],
        CPUs = config["threads_bamPEFragmentSize"]
    threads:
        config["threads_bamPEFragmentSize"]
    shell:
        """
        printf '\033[1;36m{params.sample}: Plotting the fragment size distribution...\\n\033[0m'

        bamPEFragmentSize \
        -p {params.CPUs} \
        -b {input.BAM} \
        --plotFileFormat {params.plotFormat} \
        --plotTitle {params.sample} \
        --samplesLabel {params.sample} \
        --binSize {params.binSize} \
        --blackListFileName {params.blacklist} \
        -o {output.plot}
        """


# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Computing the scaling factor
rule K_computing_scaling_factor:
    input:
        dedup_BAM_shifted_sorted = ancient(expand(os.path.join("03_BAM_dedup/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_sorted.bam"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]))),
        dedup_BAM_shifted_sorted_index = ancient(expand(os.path.join("03_BAM_dedup/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_sorted.bam.bai"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"])))
    output:
        npz_results = temp("04_Normalization/scalingFactor/scalingFactor_results.npz"),
        txt_result = "04_Normalization/scalingFactor/scalingFactor_results.txt"
    params:
        # multiBamSummary bins
        labels = ' '.join(SAMPLENAMES),
        blacklist = config["blacklist_file"],
        minFragmentLength = str(config["minFragmentLength"]),
        maxFragmentLength = str(config["maxFragmentLength"]),
        binSize = str(config["window_length"]),
        CPUs = config["multiBamSummary_threads"]
    threads:
        config["multiBamSummary_threads"]
    shell:
        """
        printf '\033[1;36mComputing the sacling factors for the intersample normalization...\\n\033[0m'

        multiBamSummary bins \
        -p {params.CPUs} \
        --bamfiles {input.dedup_BAM_shifted_sorted} \
        -o {output.npz_results} \
        --blackListFileName {params.blacklist} \
        --scalingFactors {output.txt_result} \
        --minFragmentLength {params.minFragmentLength} \
        --maxFragmentLength {params.maxFragmentLength} \
        --binSize {params.binSize} \
        --labels {params.labels}
        """
# ----------------------------------------------------------------------------------------




# ****************************************************************************************
# START OF CONDITONAL NORMALIZATION SECTION: HMCan (1) vs Sequencing deepth (2)
# ****************************************************************************************
# Run HMCan normalization if required --> perform_HMCan_correction: "False"
# ****************************************************************************************
if (eval(str(config["perform_HMCan_correction"])) == True):

    #  Generate config file for HMCcan
    rule L1_HMCan_config_file_and_genome_splitting:
        input:
            dir = ancient(os.path.dirname("04_Normalization/normalized_bigWigs/"))
        output:
            HMCan_config = "04_Normalization/HMCan_output/CONFIGURATION_file_HMCan.txt"
        params:
            # multiBamSummary bins
            labels = ' '.join(SAMPLENAMES),
            threads = str(config["multiBamSummary_threads"]),
            blacklist = config["blacklist_file"],
            minFragmentLength = str(config["minFragmentLength"]),
            maxFragmentLength = str(config["maxFragmentLength"]),
            binSize = str(config["window_length"]),
            # faidx
            genome = os.path.join(config["genome_folder"], "*.fa"),
            working_dir = config["output_directory"],
            # HMCan config
            format = config["format"],
            GCIndex = config["GCIndex"],
            smallBinLength = config["smallBinLength"],
            largeBinLength = config["largeBinLength"],
            genomePath = os.path.join(config["output_directory"], "04_Normalization/HMCan_output/temp_GENOME/"),
            pvalueThreshold = config["pvalueThreshold"],
            mergeDistance = config["mergeDistance"],
            iterationThreshold = config["iterationThreshold"],
            finalThreshold = config["finalThreshold"],
            maxIter = config["maxIter"],
            PrintWig = config["PrintWig"],
            blackListFile = config["blacklist_file"],
            PrintPosterior = config["PrintPosterior"],
            PrintBedGraph = config["PrintBedGraph"],
            CallPeaks = config["CallPeaks"],
            pairedEnds = config["pairedEnds"],
            Type = config["Type"],
            GCmergeDistance = config["GCmergeDistance"],
            RemoveDuplicates = config["RemoveDuplicates"],
            CNAnormalization = config["CNAnormalization"]
        shell:
            """
            printf '\033[1;36mSplitting the genome in single chromosomes files for HMCan normalization...\\n\033[0m'

            mkdir -p {params.genomePath}
            cd {params.genomePath}
            faidx {params.genome} --split-files
            cd {params.working_dir}



            printf '\033[1;36mReading the HMcan configuration information...\\n\033[0m'

            mkdir -p 04_Normalization/HMCan_output/

            echo format {params.format} > {output.HMCan_config}
            echo GCIndex {params.GCIndex} >> {output.HMCan_config}
            echo smallBinLength {params.smallBinLength} >> {output.HMCan_config}
            echo largeBinLength {params.largeBinLength} >> {output.HMCan_config}
            echo genomePath {params.genomePath} >> {output.HMCan_config}
            echo pvalueThreshold {params.pvalueThreshold} >> {output.HMCan_config}
            echo mergeDistance {params.mergeDistance} >> {output.HMCan_config}
            echo iterationThreshold {params.iterationThreshold} >> {output.HMCan_config}
            echo finalThreshold {params.finalThreshold} >> {output.HMCan_config}
            echo maxIter {params.maxIter} >> {output.HMCan_config}
            echo PrintWig {params.PrintWig} >> {output.HMCan_config}
            echo blackListFile {params.blackListFile} >> {output.HMCan_config}
            echo PrintPosterior {params.PrintPosterior} >> {output.HMCan_config}
            echo PrintBedGraph {params.PrintBedGraph} >> {output.HMCan_config}
            echo CallPeaks {params.CallPeaks} >> {output.HMCan_config}
            echo pairedEnds {params.pairedEnds} >> {output.HMCan_config}
            echo Type {params.Type} >> {output.HMCan_config}
            echo GCmergeDistance {params.GCmergeDistance} >> {output.HMCan_config}
            echo RemoveDuplicates {params.RemoveDuplicates} >> {output.HMCan_config}
            echo CNAnormalization {params.CNAnormalization} >> {output.HMCan_config}
            """
    # ----------------------------------------------------------------------------------------


    # ----------------------------------------------------------------------------------------
    # CNV correction by HMCan
    rule M1_signal_correction_for_CNVs_HMCan:
        input:
            dedup_BAM_shifted_sorted = ancient(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam"]))),
            dedup_BAM_shifted_sorted_index = ancient(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam.bai"]))),
            config_file = "04_Normalization/HMCan_output/CONFIGURATION_file_HMCan.txt",
        output:
            HMCan_regions = os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_regions.bed"])),
            HMCan_peaks = os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_peaks.narrowPeak"])),
            HMCan_CNV_profile = os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_CNV_profile.txt"])),
            HMCan_bedGraph = temp(os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted.bedGraph"])))
        params:
            HMCan_path = config["HMCan_path"],
            basename = os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted"])),
            sample = "{SAMPLES}"
        threads:
            config["HMCan_threads"]
        resources:
            mem_mb = 50
        shell:
            """
            printf '\033[1;36m{params.sample}: HMcan signal correction...\\n\033[0m'
            {params.HMCan_path} {input.dedup_BAM_shifted_sorted} - {input.config_file} {params.basename}
            """
    # ----------------------------------------------------------------------------------------

    # ----------------------------------------------------------------------------------------
    # Normalized bigWig generation
    rule N1_bigWig_CNV_adjusted_signal:
        input:
            HMCan_bedGraph = ancient(os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted.bedGraph"]))),
            scaling_factors = ancient("04_Normalization/scalingFactor/scalingFactor_results.txt")
        output:
            norm_bdg = temp(os.path.join("04_Normalization/HMCan_output/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_NORMALIZED.bedGraph"]))),
            norm_bw = os.path.join("04_Normalization/normalized_bigWigs/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_normalized_bs", str(config["smallBinLength"]), ".bw"]))
        params:
            sample = "{SAMPLES}",
            chrSizes = config["chromosome_sizes_file"]
        threads:
            config["HMCan_threads"]
        run:
            shell("printf '\033[1;36m{params.sample}: Normalization of the corrected signal and generation of the bigWig file...\\n\033[0m'")

            # Import bedGraph and scaling factor
            import pandas as pd
            bdg = pd.read_csv(str(input.HMCan_bedGraph),  sep='\s+', engine='python', header=None)
            factor = pd.read_csv(str(input.scaling_factors),  sep='\s+', engine='python')
            factor = factor[factor['sample']==params.sample].iloc[0,1]

            # Normalize the bedGraph scores
            bdg[3] = factor * bdg[3]

            # Export normalized_bedGraph
            bdg.to_csv(output.norm_bdg, encoding='utf-8', index=False, header=False, sep='\t')

            # Converting normalized_bedGraph to BigWig
            shell("bedGraphToBigWig {output.norm_bgd} {params.chrSizes} {output.norm_bw}")
    # ----------------------------------------------------------------------------------------



# ********************************************************************************************
else: # Skip HMCan and perform a classical "sequencing-depth" normalization
# ********************************************************************************************
    # bigWig generation from BAM (not corrected by HMCan)
    rule L2_bigWig_normalization_woCorrection:
        input:
            dedup_BAM_shifted_sorted = ancient(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam"]))),
            dedup_BAM_shifted_sorted_index = ancient(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam.bai"]))),
            scaling_factors = ancient("04_Normalization/scalingFactor/scalingFactor_results.txt")
        output:
            norm_bw = os.path.join("04_Normalization/normalized_bigWigs/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_normalized_bs", str(config["bigWig_binSize"]), ".bw"]))
        params:
            sample = "{SAMPLES}",
            binSize = config["bigWig_binSize"],
            blacklist = config["blacklist_file"],
            CPUs = config["bamCoverage_threads"]
        threads:
            config["bamCoverage_threads"]
        shell:
            """
            FACTOR=$(grep {params.sample} {input.scaling_factors} | cut -f 2)
            printf "\033[1;36m{params.sample}: generation of the normalized (x$FACTOR factor) bigWig file...\\n\033[0m"
            bamCoverage -p {params.CPUs} -b {input.dedup_BAM_shifted_sorted} --scaleFactor $FACTOR --binSize {params.binSize} --blackListFileName {params.blacklist} -of "bigwig" -o {output.norm_bw}
            """
    # ----------------------------------------------------------------------------------------


    # ----------------------------------------------------------------------------------------
    # MACS2 peakCalling on uncorrected bams
    rule M2_MACS2_peakCalling:
        input:
            dedup_BAM_shifted_sorted = ancient(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam"]))),
            dedup_BAM_shifted_sorted_index = ancient(os.path.join("03_BAM_dedup/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_sorted.bam.bai"])))
        output:
            peaks_xls = os.path.join("05_Peaks_MACS2/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_FDR", str(config["FDR_cutoff"]), "_peaks.xls"])),
            narrowPeaks = os.path.join("05_Peaks_MACS2/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_FDR", str(config["FDR_cutoff"]), "_peaks.narrowPeak"])),
            summits = os.path.join("05_Peaks_MACS2/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_FDR", str(config["FDR_cutoff"]), "_summits.bed"]))
        params:
            genomeSize = str(config["genome_size_MACS"]),
            FDR = str(config["FDR_cutoff"]),
            basename = ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_FDR", str(config["FDR_cutoff"])]),
            summits = SUMMITS,
            sample = "{SAMPLES}"
        log:
            log = os.path.join("05_Peaks_MACS2/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_FDR", str(config["FDR_cutoff"]), ".log"]))
        shell:
            """
            printf '\033[1;36m{params.sample}: Calling peaks by MACS2...\\n\033[0m'

            mkdir -p 05_Peaks_MACS2/

            macs2 callpeak \
            -t {input.dedup_BAM_shifted_sorted} \
            -g {params.genomeSize} \
	        -n {params.basename} \
            -q {params.FDR} \
            -f BAMPE \
	        --outdir 05_Peaks_MACS2 \
            --keep-dup all \
            --nolambda \
            {params.summits} 2> {log.log}
            """
    # ----------------------------------------------------------------------------------------

    # ----------------------------------------------------------------------------------------
    # HMMRATAC peakCalling on uncorrected bams
    rule N2_HMMRATAC_peakCalling:
        input:
            dedup_BAM = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bam"]))),
            dedup_BAM_index = ancient(os.path.join("03_BAM_dedup/unshifted_bams/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_sorted_woMT_dedup.bai"])))
        output:
            HMMRATAC_gappedPeaks = os.path.join("06_Peaks_HMMRATAC/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_HMMRATAC_peaks.gappedPeak"])),
            HMMRATAC_summits = os.path.join("06_Peaks_HMMRATAC/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_HMMRATAC_summits.bed"])),
            genome_file = temp(os.path.join("06_Peaks_HMMRATAC/", ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_genomeFile.info"])))
        params:
            sample = "{SAMPLES}",
            blacklist = config["blacklist_file"],
            basename = ''.join(["06_Peaks_HMMRATAC/", "{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_HMMRATAC"]),
            score_threshold = config["score_threshold"]
        log:
            ''.join(["06_Peaks_HMMRATAC/log_and_model/", "{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_HMMRATAC.log"])
        threads: 4
        shell:
            """
            printf '\033[1;36m{params.sample}: Calling peaks by HMMRATAC...\\n\033[0m'

            mkdir -p 06_Peaks_HMMRATAC/log_and_model

            samtools view -H {input.dedup_BAM} | perl -ne 'if(/^@SQ.*?SN:(\\w+)\s+LN:(\\d+)/){{print $1,"\\t",$2,"\\n"}}' > {output.genome_file}

            HMMRATAC -Xmx10G \
            --bam {input.dedup_BAM} \
            --index {input.dedup_BAM_index} \
            --genome {output.genome_file} \
            --output {params.basename} \
            --blacklist {params.blacklist} \
            --score max \
            --removeDuplicates false \
            --printTrain false \
            --threshold {params.score_threshold}

            mv {params.basename}.log {log}
            """
    # ----------------------------------------------------------------------------------------
# ****************************************************************************************
# END OF NORMALIZATION PART (if else condition)
# ****************************************************************************************


# ----------------------------------------------------------------------------------------
# Computation of the counts summary table
rule Z1_counts_summary:
    input:
        R1 = ancient(os.path.join(config["runs_directory"], "{SAMPLES}_R1.fastq.gz")),
        R2 = ancient(os.path.join(config["runs_directory"], "{SAMPLES}_R2.fastq.gz")),
        flagstat_on_unfiltered_BAM = ancient(os.path.join("02_BAM/flagstat/", "{SAMPLES}_flagstat_UNfiltered_bam.txt")),
        flagstat_on_filtered_woMT_BAM = ancient(os.path.join("02_BAM/flagstat/", "{SAMPLES}_flagstat_filtered_bam_woMT.txt")),
        dedup_BAM_flagstat = ancient(os.path.join("03_BAM_dedup/flagstat/", "{SAMPLES}_flagstat_filtered_bam_woMT_dedup.txt")),
        dedup_BAM_shifted_sorted_flagstat = ancient(os.path.join("03_BAM_dedup/flagstat/", "{SAMPLES}_flagstat_woMT_dedup_shifted_sorted.txt")),
        scaling_factors = ancient("04_Normalization/scalingFactor/scalingFactor_results.txt"),
        peaks_file = os.path.join(PEAKSDIR, ''.join(["{SAMPLES}_mapQ", str(config["mapQ_cutoff"]), "_woMT_dedup_shifted_FDR", str(config["FDR_cutoff"]), "_peaks.narrowPeak"]))
    output:
        temp_file = temp(os.path.join(SUMMARYDIR, "{SAMPLES}_counts_summary.temp"))
    params:
        R1_name = "_".join(["{SAMPLES}", "R1"]),
        R2_name = "_".join(["{SAMPLES}", "R2"]),
        sample = "{SAMPLES}",
        samples_check_string = str('|'.join(SAMPLENAMES)),
        sample_number = (len(SAMPLENAMES)),
        multiQC_report = "01_fastQC_raw/multiQC_raw/multiQC_report_fastqRaw_data/multiqc_general_stats.txt",
        peaks_dir = PEAKSDIR,
        summary_file = os.path.join(SUMMARYDIR, "counts_summary.txt")
    shell:
        """
        if test -f {params.summary_file}; then
            if [ "$(grep -E '{params.samples_check_string}' {params.summary_file} | cut -f 1 | sort --unique | wc -l | cut -f 1 -d ' ')" -gt "{params.sample_number}" ]; then
                rm {params.summary_file}
                printf '\033[1;36mGeneration of a general counts summary table...\\n\033[0m'
                printf Sample'\\t'Reads_R1'\\t'Reads_R2'\\t'Total'\\t'unfiltered_BAM'\\t'Percentage_MT'\\t'dedup_BAM'\\t'duplicated_reads'\\t'shifted_BAM'\\t'loss_post_shifting'\\t'scaling_factor'\\t'peaks_MACS2.or.HMCan'\\n' > {params.summary_file}
            else [ "$(grep -E '{params.samples_check_string}' {params.summary_file} | cut -f 1 | sort --unique | wc -l | cut -f 1 -d ' ')" -eq "{params.sample_number}" ]
                echo {params.sample} > {output.temp_file}
            fi
        else
            printf '\033[1;36mGeneration of a general counts summary table...\\n\033[0m'
            printf Sample'\\t'Reads_R1'\\t'Reads_R2'\\t'Total'\\t'unfiltered_BAM'\\t'Percentage_MT'\\t'dedup_BAM'\\t'duplicated_reads'\\t'shifted_BAM'\\t'loss_post_shifting'\\t'scaling_factor'\\t'peaks_MACS2.or.HMCan'\\n' > {params.summary_file}
        fi



        if [ "$(grep -E '{params.samples_check_string}' {params.summary_file} | cut -f 1 | sort --unique | wc -l | cut -f 1 -d ' ')" -lt "{params.sample_number}" ]; then
            printf '\033[1;36mGeneration of a general counts summary table...\\n\033[0m'

            R1=$(grep {params.R1_name} {params.multiQC_report} | cut -f 6 | sed 's/\.[0-9]\+//')
            R2=$(grep {params.R2_name} {params.multiQC_report} | cut -f 6 | sed 's/\.[0-9]\+//')
            TOTAL=$((R1 + R2))

            unfilteredBAM=$(grep mapped {input.flagstat_on_unfiltered_BAM} | head -n 1 | cut -f 1 -d ' ')
            woMT_BAM=$(grep mapped {input.flagstat_on_filtered_woMT_BAM} | head -n 1 | cut -f 1 -d ' ')
            percMT=$(echo "scale=1; (100 - (($woMT_BAM/$unfilteredBAM) * 100))" | bc)

            dedupBAM=$(grep mapped {input.dedup_BAM_flagstat} | head -n 1 | cut -f 1 -d ' ')
            dedupREADS=$((woMT_BAM - dedupBAM))

            shiftedBAM=$(grep mapped {input.dedup_BAM_shifted_sorted_flagstat} | head -n 1 | cut -f 1 -d ' ')
            lossReads=$((dedupBAM - shiftedBAM))

            FACTOR=$(grep {params.sample} {input.scaling_factors} | cut -f 2)

            peaks=$(wc -l {params.peaks_dir}{params.sample}*.*Peak | cut -f 1 -d ' ')

            printf {params.sample}'\\t'$R1'\\t'$R2'\\t'$TOTAL'\\t'$unfilteredBAM'\\t'$percMT'\\t'$dedupBAM'\\t'$dedupREADS'\\t'$shiftedBAM'\\t'$lossReads'\\t'$FACTOR'\\t'$peaks'\\n' >> {params.summary_file}
            printf {params.sample}'\\t'$R1'\\t'$R2'\\t'$TOTAL'\\t'$unfilteredBAM'\\t'$percMT'\\t'$dedupBAM'\\t'$dedupREADS'\\t'$shiftedBAM'\\t'$lossReads'\\t'$FACTOR'\\t'$peaks'\\n' > {output.temp_file}

            uniq -u {params.summary_file} > summary_file.temp
            (head -n 1 summary_file.temp && tail -n +2 summary_file.temp | sort -k 1) > {params.summary_file}
            rm summary_file.temp
        fi
        """


# ----------------------------------------------------------------------------------------
# Generation of samples PCA and Heatmap
rule Z2_PCA_and_samples_correlation:
    input:
        norm_bw = ancient(expand(os.path.join("04_Normalization/normalized_bigWigs/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_normalized_bs{binSize}.bw"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), binSize=str(BINS)))
    output:
        matrix = temp(os.path.join(SUMMARYDIR, "temp_multiBigWigSummary_matrix.npz")),
        PCA = os.path.join(SUMMARYDIR, "PCA_on_BigWigs_wholeGenome.pdf"),
        hetamap_spearman = os.path.join(SUMMARYDIR, "Heatmap_on_BigWigs_wholeGenome_spearmanMethod.pdf"),
        hetamap_pearson = os.path.join(SUMMARYDIR, "Heatmap_on_BigWigs_wholeGenome_pearsonMethod.pdf"),
        scatterplot_spearman = os.path.join(SUMMARYDIR, "Scatterplot_on_BigWigs_wholeGenome_spearmanMethod.pdf"),
        scatterplot_pearson = os.path.join(SUMMARYDIR, "Scatterplot_on_BigWigs_wholeGenome_pearsonMethod.pdf")
    params:
        output_dir = SUMMARYDIR,
        labels = ' '.join(SAMPLENAMES),
        window = config["binning_window_size"],
        blacklist = config["blacklist_file"],
        heatmap_color = config["heatmap_color"],
        CPUs = config["threads_multiBigwigSummary"]
    threads:
        config["threads_multiBigwigSummary"]
    shell:
        """
        printf '\033[1;36mComputing the correlation and variability of the whole signal among samples...\\n\033[0m'

        multiBigwigSummary bins -p {params.CPUs} -b {input.norm_bw} --labels {params.labels} --binSize {params.window} --blackListFileName {params.blacklist} -o {output.matrix}


        printf '\033[1;36m    - plotting PCA...\\n\033[0m'
        plotPCA -in {output.matrix} -o {output.PCA} -T 'PCA on BigWigs (whole genome)' --plotFileFormat 'pdf'


        printf '\033[1;36m    - plotting Spearman correlation heatmap...\\n\033[0m'
        plotCorrelation -in {output.matrix} \
        --corMethod spearman \
        --skipZeros \
        --removeOutliers \
        --plotTitle "Spearman correlation of BigWigs" \
        --whatToPlot heatmap \
        --colorMap {params.heatmap_color} \
        --plotNumbers \
        --plotFileFormat 'pdf' \
        -o {output.hetamap_spearman}

        printf '\033[1;36m    - plotting Pearson correlation heatmap...\\n\033[0m'
        plotCorrelation -in {output.matrix} \
        --corMethod pearson \
        --skipZeros \
        --removeOutliers \
        --plotTitle "Pearson correlation of BigWigs" \
        --whatToPlot heatmap \
        --colorMap {params.heatmap_color} \
        --plotNumbers \
        --plotFileFormat 'pdf' \
        -o {output.hetamap_pearson}



        printf '\033[1;36m    - plotting Spearman correlation scatterplot...\\n\033[0m'
        plotCorrelation -in {output.matrix} \
        --corMethod spearman \
        --log1p \
        --skipZeros \
        --removeOutliers \
        --plotTitle "Spearman correlation of BigWigs - ln values" \
        --whatToPlot scatterplot \
        --colorMap {params.heatmap_color} \
        --plotNumbers \
        --plotFileFormat 'pdf' \
        -o {output.scatterplot_spearman}

        printf '\033[1;36m    - plotting Pearson correlation scatterplot...\\n\033[0m'
        plotCorrelation -in {output.matrix} \
        --corMethod pearson \
        --log1p \
        --skipZeros \
        --removeOutliers \
        --plotTitle "Pearson correlation of BigWigs - ln values" \
        --whatToPlot scatterplot \
        --colorMap {params.heatmap_color} \
        --plotNumbers \
        --plotFileFormat 'pdf' \
        -o {output.scatterplot_pearson}
        """


# ----------------------------------------------------------------------------------------
# Absolute peaks file and relative matrix score generation for MACS2 peaks
rule Z3_all_peaks_file_and_score_matrix__MACS2_HMcan:
    input:
        norm_bw = ancient(expand(os.path.join("04_Normalization/normalized_bigWigs/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_normalized_bs{binSize}.bw"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), binSize=str(BINS))),
        peaks_file = (expand(os.path.join(str(PEAKSDIR), "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_FDR{fdr}_peaks.narrowPeak"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), fdr=str(config["FDR_cutoff"])))
    output:
        concatenation_bed = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation.bed")),
        concatenation_bed_sorted = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation_sorted.bed")),
        concatenation_bed_collapsed = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation_collapsed.bed")),
        concatenation_bed_collapsed_sorted = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation_collapsed_sorted.bed")),
        score_matrix_peaks = temp(os.path.join(SUMMARYDIR, "temp_peaks_score_matrix_all_samples_MACS2.npz")),
        score_matrix_peaks_table = temp(os.path.join(SUMMARYDIR, "temp_peaks_score_matrix_all_samples_table_{PEAKCALLER}.tsv"))
    params:
        peaks_dir = PEAKSDIR,
        labels = ' '.join(SAMPLENAMES),
        blacklist = config["blacklist_file"],
        CPUs = config["threads_multiBigwigSummary"]
    threads:
        config["threads_multiBigwigSummary"]
    shell:
        """
        printf '\033[1;36mGenerating a file result of the merge of all the MACS2 peaks...\\n\033[0m'
        cat {params.peaks_dir}*.*Peak >> {output.concatenation_bed}
        sort -V -k1,1 -k2,2 -k5,5 {output.concatenation_bed} > {output.concatenation_bed_sorted}

        bedtools merge -i {output.concatenation_bed_sorted} | uniq > {output.concatenation_bed_collapsed}
        sort -V -k1,1 -k2,2 -k5,5 {output.concatenation_bed_collapsed} > {output.concatenation_bed_collapsed_sorted}


        printf '\033[1;36mComputing the score matrix for all the MACS2 peaks per each sample...\\n\033[0m'

        multiBigwigSummary BED-file \
        -p {params.CPUs} \
        -b {input.norm_bw} \
        -o {output.score_matrix_peaks} \
        --BED {output.concatenation_bed_collapsed_sorted} \
        --blackListFileName {params.blacklist} \
        --outRawCounts {output.score_matrix_peaks_table} \
        --labels {params.labels}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Compute peaks z-scores and plot heatmap for MACS2/HMCan peaks
rule Z4_peaks_zScores_and_heatmap__MACS2_HMCan:
    input:
        score_matrix_peaks_table = ancient(os.path.join(SUMMARYDIR, "temp_peaks_score_matrix_all_samples_table_{PEAKCALLER}.tsv"))
    output:
        rawScores_hetamap = os.path.join(SUMMARYDIR, "Heatmap_on_rawScores_for_{PEAKCALLER}.peaks_union_population.pdf")
    params:
        heatmap_color = config["heatmap_color"],
        heatmap_basename_rawScores = os.path.join(SUMMARYDIR, "Heatmap_on_rawScores_for_{PEAKCALLER}.peaks_union_population"),
        heatmap_basename_zScore = os.path.join(SUMMARYDIR, "Heatmap_on_zScores_for_{PEAKCALLER}.peaks_union_population"),
        n_samples = len(SAMPLENAMES),
        peak_caller = PEAKCALLER
    run:
        # Messege
        shell("printf '\033[1;36mPlotting the {PEAKCALLER} peak score hetamaps...\\n\033[0m'")

        # Import multiBigWig summary table
        import pandas as pd
        matrix = pd.read_csv(str(input.score_matrix_peaks_table),  sep='\s+', engine='python')

        # Use peak coordinates as ID ofr each row
        matrix["peak_ID"] = matrix[matrix.columns[:3]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)
        matrix = matrix[matrix.columns[3:]]
        matrix = matrix.set_index('peak_ID')

        # Required to avoid the use of X11
        import matplotlib as mpl
        mpl.use('Agg')
        import matplotlib.pyplot as plt

        # Required to avoid errors in scipy iterations
        import sys
        sys.setrecursionlimit(100000)

        # Generation of the rawScore heatmap and clustering
        from bioinfokit import analys, visuz
        visuz.gene_exp.hmap(df=matrix,
                            cmap=params.heatmap_color,
                            rowclus=True,
                            colclus=(params.n_samples > 1),
                            figtype="pdf",
                            ylabel=False,
                            figname=str(params.heatmap_basename_rawScores),
                            dim=(4.5, 9),
                            tickfont=(6, 4))


        ## Generation of the zScore heatmap and clustering
        #from bioinfokit import analys, visuz
        #visuz.gene_exp.hmap(df=matrix,
                            #cmap=params.heatmap_color,
                            #rowclus=True,
                            #colclus=True,
                            #zscore=0,
                            #figtype ="pdf",
                            #ylabel=False,
                            #figname=str(params.heatmap_basename_zScore),
                            #dim=(6, 12),
                            #tickfont=(6, 4))

        # ---------------- Manual way to compute zScore heatmap Z=(rowScore - rowMean)/rowSD -----------------
        if params.n_samples > 1:
            import pandas as pd
            matrix = pd.read_csv(str(input.score_matrix_peaks_table),  sep='\s+', engine='python')
            stat_tb = pd.DataFrame({'rowMeans': matrix[matrix.columns[3:]].mean(axis=1),
                                    'SD':  matrix[matrix.columns[3:]].std(axis=1)})

            scores = []
            for i in list(range(3,len(matrix.columns))):
                scores.append((matrix[matrix.columns[i]] - stat_tb["rowMeans"]) / stat_tb["SD"])


            zScores = pd.DataFrame(scores).transpose()
            zScores.columns = list(matrix.columns)[3:len(matrix.columns)]
            zScores = zScores.fillna(0)
            zScores['peak_ID'] = matrix[matrix.columns[:3]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)
            zScores = zScores.set_index('peak_ID')

            visuz.gene_exp.hmap(df=zScores,
                                cmap=params.heatmap_color,
                                rowclus=True,
                                colclus=True,
                                figtype ="pdf",
                                ylabel=False,
                                figname=str(params.heatmap_basename_zScore),
                                dim=(4.5, 9),
                                tickfont=(6, 4))
        #------- ------- ------- ------- ------- ------- ------- ------- ------- -------
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Absolute peaks file and relative matrix score generation for HMMRATAC peaks
rule Z5_all_peaks_file_and_score_matrix__HMMRATAC:
    input:
        norm_bw = ancient(expand(os.path.join("04_Normalization/normalized_bigWigs/", "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_normalized_bs{binSize}.bw"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"]), binSize=str(BINS))),
        peaks_file = (expand(os.path.join(str(PEAKSHMMRATAC), "{sample}_mapQ{MAPQ}_woMT_dedup_shifted_HMMRATAC_peaks.gappedPeak"), sample=SAMPLENAMES, MAPQ=str(config["mapQ_cutoff"])))
    output:
        concatenation_bed = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation.bed")),
        concatenation_bed_sorted = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation_sorted.bed")),
        concatenation_bed_collapsed = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation_collapsed.bed")),
        concatenation_bed_collapsed_sorted = temp(os.path.join(SUMMARYDIR, "temp_all_samples_peaks_concatenation_collapsed_sorted.bed")),
        score_matrix_peaks = temp(os.path.join(SUMMARYDIR, "temp_peaks_score_matrix_all_samples_HMMRATAC.npz")),
        score_matrix_peaks_table = temp(os.path.join(SUMMARYDIR, "temp_peaks_score_matrix_all_samples_table_HMMRATAC.tsv"))
    params:
        peaks_dir = PEAKSHMMRATAC,
        labels = ' '.join(SAMPLENAMES),
        blacklist = config["blacklist_file"],
        CPUs = config["threads_multiBigwigSummary"]
    threads:
        config["threads_multiBigwigSummary"]
    shell:
        """
        printf '\033[1;36mGenerating a file result of the merge of all the HMMRATAC peaks...\\n\033[0m'
        cat {params.peaks_dir}*.*Peak >> {output.concatenation_bed}
        sort -V -k1,1 -k2,2 -k5,5 {output.concatenation_bed} > {output.concatenation_bed_sorted}

        bedtools merge -i {output.concatenation_bed_sorted} | uniq > {output.concatenation_bed_collapsed}
        sort -V -k1,1 -k2,2 -k5,5 {output.concatenation_bed_collapsed} > {output.concatenation_bed_collapsed_sorted}

        printf '\033[1;36mComputing the score matrix for all the HMMRATAC peaks per each sample...\\n\033[0m'

        multiBigwigSummary BED-file \
        -p {params.CPUs} \
        -b {input.norm_bw} \
        -o {output.score_matrix_peaks} \
        --BED {output.concatenation_bed_collapsed_sorted} \
        --blackListFileName {params.blacklist} \
        --outRawCounts {output.score_matrix_peaks_table} \
        --labels {params.labels}
        """
# ----------------------------------------------------------------------------------------


# ----------------------------------------------------------------------------------------
# Compute peaks z-scores and plot heatmap
rule Z6_peaks_zScores_and_heatmap__HMMRATAC:
    input:
        score_matrix_peaks_table_HMMRATAC = ancient(os.path.join(SUMMARYDIR, "temp_peaks_score_matrix_all_samples_table_HMMRATAC.tsv"))
    output:
        rawScores_hetamap_HMMRATAC = os.path.join(SUMMARYDIR, "Heatmap_on_rawScores_for_HMMRATAC.peaks_union_population.pdf")
    params:
        heatmap_color = config["heatmap_color"],
        heatmap_basename_rawScores_HMMRATAC = os.path.join(SUMMARYDIR, "Heatmap_on_rawScores_for_HMMRATAC.peaks_union_population"),
        heatmap_basename_zScore_HMMRATAC = os.path.join(SUMMARYDIR, "Heatmap_on_zScores_for_HMMRATAC.peaks_union_population"),
        n_samples = len(SAMPLENAMES)
    run:
        # Messege
        shell("printf '\033[1;36mPlotting the peak score hetamaps...\\n\033[0m'")

        # Import multiBigWig summary table
        import pandas as pd
        matrix = pd.read_csv(str(input.score_matrix_peaks_table_HMMRATAC),  sep='\s+', engine='python')

        # Use peak coordinates as ID ofr each row
        matrix["peak_ID"] = matrix[matrix.columns[:3]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)
        matrix = matrix[matrix.columns[3:]]
        matrix = matrix.set_index('peak_ID')

        # Required to avoid the use of X11
        import matplotlib as mpl
        mpl.use('Agg')
        import matplotlib.pyplot as plt

        # Required to avoid errors in scipy iterations
        import sys
        sys.setrecursionlimit(100000)

        # Generation of the rawScore heatmap and clustering
        from bioinfokit import analys, visuz
        visuz.gene_exp.hmap(df=matrix,
                            cmap=params.heatmap_color,
                            rowclus=True,
                            colclus=(params.n_samples > 1),
                            figtype="pdf",
                            ylabel=False,
                            figname=str(params.heatmap_basename_rawScores_HMMRATAC),
                            dim=(4.5, 9),
                            tickfont=(6, 4))


        ## Generation of the zScore heatmap and clustering
        #from bioinfokit import analys, visuz
        #visuz.gene_exp.hmap(df=matrix,
                            #cmap=params.heatmap_color,
                            #rowclus=True,
                            #colclus=True,
                            #zscore=0,
                            #figtype ="pdf",
                            #ylabel=False,
                            #figname=str(params.heatmap_basename_zScore),
                            #dim=(6, 12),
                            #tickfont=(6, 4))

        # ---------------- Manual way to compute zScore heatmap Z=(rowScore - rowMean)/rowSD -----------------
        if params.n_samples > 1:
            import pandas as pd
            matrix = pd.read_csv(str(input.score_matrix_peaks_table_HMMRATAC),  sep='\s+', engine='python')
            stat_tb = pd.DataFrame({'rowMeans': matrix[matrix.columns[3:]].mean(axis=1),
                                    'SD':  matrix[matrix.columns[3:]].std(axis=1)})

            scores = []
            for i in list(range(3,len(matrix.columns))):
                scores.append((matrix[matrix.columns[i]] - stat_tb["rowMeans"]) / stat_tb["SD"])


            zScores = pd.DataFrame(scores).transpose()
            zScores.columns = list(matrix.columns)[3:len(matrix.columns)]
            zScores = zScores.fillna(0)
            zScores['peak_ID'] = matrix[matrix.columns[:3]].apply(lambda x: '_'.join(x.dropna().astype(str)),axis=1)
            zScores = zScores.set_index('peak_ID')

            visuz.gene_exp.hmap(df=zScores,
                                cmap=params.heatmap_color,
                                rowclus=True,
                                colclus=True,
                                figtype ="pdf",
                                ylabel=False,
                                figname=str(params.heatmap_basename_zScore_HMMRATAC),
                                dim=(4.5, 9),
                                tickfont=(6, 4))
        #------- ------- ------- ------- ------- ------- ------- ------- ------- -------
# ----------------------------------------------------------------------------------------
